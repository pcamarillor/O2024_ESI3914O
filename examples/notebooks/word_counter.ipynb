{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcamarillor/O2024_ESI3914O/blob/pablo_camarillo_add_spark_setup/examples/notebooks/lab01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPSQzDhW0nKI",
        "outputId": "6bfe432e-7678-42d6-8232-0a25c3496a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Password:\n",
            "sudo: a password is required\n",
            "zsh:1: command not found: apt-get\n",
            "zsh:1: command not found: wget\n",
            "tar: Error opening archive: Failed to open 'spark-3.5.2-bin-hadoop3.tgz'\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812371 sha256=bedaf3f1adc3da915ba4872a55453dd2abbfd0771c7fad90ddd21021f38f03fc\n",
            "  Stored in directory: /Users/miguelmendez/Library/Caches/pip/wheels/9d/29/ee/3a756632ca3f0a6870933bac1c9db6e4af2c068f019aba0ee1\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.7 pyspark-3.5.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: py4j in /Users/miguelmendez/anaconda3/lib/python3.11/site-packages (0.10.9.7)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Check this site for the latest download link\n",
        "# https://www.apache.org/dyn/closer.lua/spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c8RNY4wS0iKl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/08/30 11:45:58 WARN Utils: Your hostname, Miguels-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.0.0.2 instead (on interface en0)\n",
            "24/08/30 11:45:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/08/30 11:45:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "  .master(\"local[*]\") \\\n",
        "  .appName(\"ITESO-2024-SparkIntroduction\") \\\n",
        "  .config(\"spark.driver.bindAddress\",\"localhost\") \\\n",
        "  .config(\"spark.ui.port\",\"4040\") \\\n",
        "  .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCFQkD7M0iKm",
        "outputId": "7fe1bad8-4b4a-4c0f-f822-2bd79a4dab84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Lorem', 'ipsum', 'dolor', 'sit', 'amet,', 'consectetur', 'adipiscing', 'elit.', 'Sed', 'do', 'eiusmod', 'tempor', 'incididunt', 'ut', 'labore', 'et', 'dolore', 'magna', 'aliqua.', 'Ut', 'enim', 'ad', 'minim', 'veniam,', 'quis', 'nostrud', 'exercitation', 'ullamco', 'laboris', 'nisi', 'ut', 'aliquip', 'ex', 'ea', 'commodo', 'consequat.', 'Duis', 'aute', 'irure', 'dolor', 'in', 'reprehenderit', 'in', 'voluptate', 'velit', 'esse', 'cillum', 'dolore', 'eu', 'fugiat', 'nulla', 'pariatur.', 'Excepteur', 'sint', 'occaecat', 'cupidatat', 'non', 'proident,', 'sunt', 'in', 'culpa', 'qui', 'officia', 'deserunt', 'mollit', 'anim', 'id', 'est', 'laborum.', 'Curabitur', 'pretium', 'tincidunt', 'lacus.', 'Nulla', 'gravida', 'orci', 'a', 'odio.', 'Nullam', 'varius,', 'turpis', 'et', 'commodo', 'pharetra,', 'est', 'eros', 'bibendum', 'elit,', 'nec', 'malesuada', 'elit', 'elit', 'vel', 'lectus.', 'Sed', 'ut', 'perspiciatis', 'unde', 'omnis', 'iste', 'natus', 'error', 'sit', 'voluptatem', 'accusantium', 'doloremque', 'laudantium.', 'Nemo', 'enim', 'ipsam', 'voluptatem', 'quia', 'voluptas', 'sit', 'aspernatur', 'aut', 'odit', 'aut', 'fugit.', 'Sed', 'quia', 'consequuntur', 'magni', 'dolores', 'eos', 'qui', 'ratione', 'voluptatem', 'sequi', 'nesciunt.', 'Neque', 'porro', 'quisquam', 'est', 'qui', 'dolorem', 'ipsum', 'quia', 'dolor', 'sit', 'amet,', 'consectetur,', 'adipisci', 'velit.', 'Lorem', 'ipsum', 'dolor', 'sit', 'amet,', 'consectetur', 'adipiscing', 'elit.', 'Sed', 'do', 'eiusmod', 'tempor', 'incididunt', 'ut', 'labore', 'et', 'dolore', 'magna', 'aliqua.', 'Ut', 'enim', 'ad', 'minim', 'veniam,', 'quis', 'nostrud', 'exercitation', 'ullamco', 'laboris', 'nisi', 'ut', 'aliquip', 'ex', 'ea', 'commodo', 'consequat.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Create an RDD with a list of sentences\n",
        "sentences = sentences = [\n",
        "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n",
        "    \"Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\",\n",
        "    \"Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\",\n",
        "    \"Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\",\n",
        "    \"Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\",\n",
        "    \"Curabitur pretium tincidunt lacus. Nulla gravida orci a odio.\",\n",
        "    \"Nullam varius, turpis et commodo pharetra, est eros bibendum elit, nec malesuada elit elit vel lectus.\",\n",
        "    \"Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium.\",\n",
        "    \"Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit.\",\n",
        "    \"Sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt.\",\n",
        "    \"Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit.\",\n",
        "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\",\n",
        "    \"Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\"\n",
        "]\n",
        "sentences_rdd = sc.parallelize(sentences)\n",
        "\n",
        "# Tokenize the sentences into words\n",
        "words_rdd = sentences_rdd.flatMap(lambda line: line.split())\n",
        "\n",
        "print(words_rdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOU3eBKy1XCh",
        "outputId": "cab1f8ae-d051-453f-c153-6408f373e393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('magna', 2), ('aliqua.', 2), ('veniam,', 2), ('quis', 2), ('nisi', 2), ('irure', 1), ('qui', 3), ('pretium', 1), ('tincidunt', 1), ('odio.', 1), ('turpis', 1), ('nec', 1), ('perspiciatis', 1), ('consectetur,', 1), ('ipsum', 3), ('laboris', 2), ('in', 3), ('eu', 1), ('cupidatat', 1), ('bibendum', 1), ('elit', 2), ('iste', 1), ('dolores', 1), ('Neque', 1), ('dolor', 4), ('adipiscing', 2), ('eiusmod', 2), ('Ut', 2), ('exercitation', 2), ('ullamco', 2), ('ea', 2), ('Duis', 1), ('aute', 1), ('velit', 1), ('esse', 1), ('id', 1), ('est', 3), ('vel', 1), ('unde', 1), ('aspernatur', 1), ('porro', 1), ('labore', 2), ('aliquip', 2), ('ex', 2), ('officia', 1), ('orci', 1), ('a', 1), ('laudantium.', 1), ('quia', 3), ('fugit.', 1), ('nesciunt.', 1), ('quisquam', 1), ('Lorem', 2), ('incididunt', 2), ('ut', 5), ('dolore', 3), ('enim', 3), ('cillum', 1), ('nulla', 1), ('deserunt', 1), ('anim', 1), ('Nulla', 1), ('omnis', 1), ('error', 1), ('eos', 1), ('ratione', 1), ('dolorem', 1), ('sit', 5), ('elit.', 2), ('tempor', 2), ('nostrud', 2), ('pariatur.', 1), ('Excepteur', 1), ('sint', 1), ('occaecat', 1), ('proident,', 1), ('sunt', 1), ('culpa', 1), ('mollit', 1), ('lacus.', 1), ('gravida', 1), ('eros', 1), ('elit,', 1), ('malesuada', 1), ('consequuntur', 1), ('magni', 1), ('adipisci', 1), ('et', 3), ('commodo', 3), ('fugiat', 1), ('non', 1), ('laborum.', 1), ('Curabitur', 1), ('lectus.', 1), ('natus', 1), ('doloremque', 1), ('Nemo', 1), ('aut', 2), ('amet,', 3), ('consectetur', 2), ('Sed', 4), ('do', 2), ('ad', 2), ('minim', 2), ('consequat.', 2), ('reprehenderit', 1), ('voluptate', 1), ('Nullam', 1), ('varius,', 1), ('pharetra,', 1), ('voluptatem', 3), ('accusantium', 1), ('ipsam', 1), ('voluptas', 1), ('odit', 1), ('sequi', 1), ('velit.', 1)]\n"
          ]
        }
      ],
      "source": [
        "# Compute the frequency of each word\n",
        "word_counts_rdd = words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "print(word_counts_rdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOvhnPL61g51",
        "outputId": "9aea2f68-7fc9-4546-9394-15395126b3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most common word: [('ut', 5)]\n"
          ]
        }
      ],
      "source": [
        "# Find the most common word\n",
        "most_common_word = word_counts_rdd.takeOrdered(1, key=lambda x: -x[1])\n",
        "print(\"Most common word:\", most_common_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCIztSBe1nWp",
        "outputId": "1fc6e3fc-a3d8-4658-8493-e9876e08e9f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average word length: 5.605555555555555\n"
          ]
        }
      ],
      "source": [
        "# Compute the average word length\n",
        "total_length_rdd = words_rdd.map(lambda word: len(word)).reduce(lambda a, b: a + b)\n",
        "total_words = words_rdd.count()\n",
        "average_word_length = total_length_rdd / total_words if total_words > 0 else 0\n",
        "print(\"Average word length:\", average_word_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longest word: reprehenderit\n"
          ]
        }
      ],
      "source": [
        "# Get the longest word in the dataset\n",
        "longest_word = words_rdd.sortBy(lambda word: len(word), ascending=False).first()\n",
        "print(\"Longest word:\", longest_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shortest word: a\n"
          ]
        }
      ],
      "source": [
        "# Get the shortes word in the dataset\n",
        "shortest_word = words_rdd.sortBy(lambda word: len(word), ascending=True).first()\n",
        "print(\"Shortest word:\", shortest_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fmbP4gR_1p1q"
      },
      "outputs": [],
      "source": [
        "# Stop the SparkContext\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
