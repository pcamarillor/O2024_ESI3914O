{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pcamarillor/O2024_ESI3914O/blob/pablo_camarillo_add_spark_setup/examples/notebooks/lab01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPSQzDhW0nKI",
    "outputId": "6bfe432e-7678-42d6-8232-0a25c3496a2b"
   },
   "source": [
    "!apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# Check this site for the latest download link\n",
    "# https://www.apache.org/dyn/closer.lua/spark\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.2-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "!pip install py4j"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c8RNY4wS0iKl",
    "ExecuteTime": {
     "end_time": "2024-08-28T03:34:54.766904Z",
     "start_time": "2024-08-28T03:34:54.761531Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "  .master(\"local[*]\") \\\n",
    "  .appName(\"ITESO-2024-SparkIntroduction\") \\\n",
    "  .config(\"spark.driver.bindAddress\",\"localhost\") \\\n",
    "  .config(\"spark.ui.port\",\"4040\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCFQkD7M0iKm",
    "outputId": "7fe1bad8-4b4a-4c0f-f822-2bd79a4dab84"
   },
   "source": [
    "# Create an RDD with a list of sentences\n",
    "sentences = [\n",
    "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",\n",
    "    \"Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\",\n",
    "    \"Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\",\n",
    "    \"Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\",\n",
    "    \"Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\",\n",
    "    \"Curabitur pretium tincidunt lacus. Nulla gravida orci a odio.\",\n",
    "    \"Nullam varius, turpis et commodo pharetra, est eros bibendum elit, nec malesuada elit elit vel lectus.\",\n",
    "    \"Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium.\",\n",
    "    \"Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit.\",\n",
    "    \"Sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt.\",\n",
    "    \"Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit.\",\n",
    "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\",\n",
    "    \"Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\"\n",
    "]\n",
    "sentences_rdd = sc.parallelize(sentences)\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "words_rdd = sentences_rdd.flatMap(lambda line: line.split())\n",
    "\n",
    "print(words_rdd.collect())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOU3eBKy1XCh",
    "outputId": "cab1f8ae-d051-453f-c153-6408f373e393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Spark', 2), ('is', 1), ('unified', 1), ('analytics', 1), ('engine', 1), ('It', 1), ('provides', 1), ('general-purpose', 1), ('cluster-computing', 1), ('framework', 1), ('are', 1), ('fundamental', 1), ('supports', 1), ('Python', 1), ('a', 2), ('fast', 1), ('and', 2), ('RDDs', 1), ('the', 1), ('data', 1), ('structure', 1), ('many', 1), ('languages', 1), ('including', 1), ('Scala', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency of each word\n",
    "word_counts_rdd = words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "print(word_counts_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOvhnPL61g51",
    "outputId": "9aea2f68-7fc9-4546-9394-15395126b3cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common word: [('Spark', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Find the most common word\n",
    "most_common_word = word_counts_rdd.takeOrdered(1, key=lambda x: -x[1])\n",
    "print(\"Most common word:\", most_common_word)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 6.142857142857143\n"
     ]
    }
   ],
   "execution_count": 12,
   "source": [
    "# Compute the average word length\n",
    "total_length_rdd = words_rdd.map(lambda word: len(word)).reduce(lambda a, b: a + b)\n",
    "total_words = words_rdd.count()\n",
    "average_word_length = total_length_rdd / total_words if total_words > 0 else 0\n",
    "print(\"Average word length:\", average_word_length)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sort the RDD by word length in ascending order\n",
    "sorted_words_by_length = words_rdd.sortBy(lambda word: len(word))\n",
    "\n",
    "# Get the shortest word\n",
    "shortest_word = sorted_words_by_length.first()\n",
    "\n",
    "# Get the longest word\n",
    "longest_word = sorted_words_by_length.sortBy(lambda word: len(word), ascending=False).first()\n",
    "\n",
    "# Print the results\n",
    "print(\"Shortest word:\", shortest_word)\n",
    "print(\"Longest word:\", longest_word)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Alternative solution using reduce\n",
    "# Find the shortest word\n",
    "shortest_word = words_rdd.reduce(lambda a, b: a if len(a) < len(b) else b)\n",
    "# Find the longest word\n",
    "longest_word = words_rdd.reduce(lambda a, b: a if len(a) > len(b) else b)\n",
    "\n",
    "# Print the results\n",
    "print(\"Shortest word:\", shortest_word)\n",
    "print(\"Longest word:\", longest_word)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 13,
   "source": [
    "# Stop the SparkContext\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
