{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pcamarillor/O2024_ESI3914O/blob/Lab10_Team6/Copia_de_trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_HOIjKbSbdd",
    "outputId": "eb9cf75d-9e2d-4148-f5f2-82fd8a2a3af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-XD33tTpSwIy",
    "outputId": "067f1469-ee17-401b-aab9-37a6f6a7a62f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,611 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,456 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,397 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,672 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,163 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,241 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,452 kB]\n",
      "Fetched 22.4 MB in 3s (6,827 kB/s)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "50 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
      "tar: spark-3.5.2-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n",
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# Check this site for the latest download link\n",
    "# https://www.apache.org/dyn/closer.lua/spark\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.2-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "!pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4jLvx82LS38t",
    "outputId": "b8f9efdf-3b61-4e83-dd38-b2a4b590855a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- SepalLengthCm: float (nullable = true)\n",
      " |-- SepalWidthCm: float (nullable = true)\n",
      " |-- PetalLengthCm: float (nullable = true)\n",
      " |-- PetalWidthCm: float (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|Id |SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species    |\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|1  |5.1          |3.5         |1.4          |0.2         |Iris-setosa|\n",
      "|2  |4.9          |3.0         |1.4          |0.2         |Iris-setosa|\n",
      "|3  |4.7          |3.2         |1.3          |0.2         |Iris-setosa|\n",
      "|4  |4.6          |3.1         |1.5          |0.2         |Iris-setosa|\n",
      "|5  |5.0          |3.6         |1.4          |0.2         |Iris-setosa|\n",
      "|6  |5.4          |3.9         |1.7          |0.4         |Iris-setosa|\n",
      "|7  |4.6          |3.4         |1.4          |0.3         |Iris-setosa|\n",
      "|8  |5.0          |3.4         |1.5          |0.2         |Iris-setosa|\n",
      "|9  |4.4          |2.9         |1.4          |0.2         |Iris-setosa|\n",
      "|10 |4.9          |3.1         |1.5          |0.1         |Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create Spark Session in localhost\n",
    "spark = SparkSession.builder.master(\"local\").\\\n",
    "    appName(\"ITESO-LogisticRegression\").\\\n",
    "    getOrCreate()\n",
    "\n",
    "# Set spark context\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "data_path = \"/my_drive_path/Iris.csv\"  # Cambia la ruta si es necesario\n",
    "iris_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "iris_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vFMM3u0gX_ec",
    "outputId": "e1b12dbe-74c6-4af5-9c12-d8bdba3c49e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- SepalLengthCm: float (nullable = true)\n",
      " |-- SepalWidthCm: float (nullable = true)\n",
      " |-- PetalLengthCm: float (nullable = true)\n",
      " |-- PetalWidthCm: float (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n",
    "data = label_indexer.fit(iris_df).transform(iris_df)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wzVDa_vHVH7R"
   },
   "outputs": [],
   "source": [
    "# Assemble the features into a single vector column\n",
    "columns = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\n",
    "assembler = VectorAssembler(inputCols=columns, outputCol=\"features\")\n",
    "\n",
    "data_with_features = assembler.transform(data).select(\"label\", \"features\")\n",
    "\n",
    "# Split the data: 80% training data and 20% testing data\n",
    "train, test = data_with_features.randomSplit([0.8, 0.2], seed=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-WuxnLz1ZvVn"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cUmLFijVPhJ",
    "outputId": "4f9642c8-1d29-44f0-d451-b4038a96a45c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9629629629629629\n",
      "Precision: 0.9675925925925926\n",
      "Recall: 0.9629629629629629\n",
      "F1 Score: 0.9632228719948018\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "dt_model = dt.fit(train)\n",
    "\n",
    "predictions = dt_model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "precision = evaluator.evaluate(predictions,\n",
    "{evaluator.metricName: \"weightedPrecision\"})\n",
    "print(f\"Precision: {precision}\")\n",
    "recall = evaluator.evaluate(predictions,\n",
    "{evaluator.metricName: \"weightedRecall\"})\n",
    "print(f\"Recall: {recall}\")\n",
    "f1 = evaluator.evaluate(predictions,\n",
    "{evaluator.metricName: \"f1\"})\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOSTPI9B48Ac",
    "outputId": "e85d7546-8164-4834-d0f0-7f0c7990e1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model summary:DecisionTreeClassificationModel: uid=DecisionTreeClassifier_a494a12d5c65, depth=5, numNodes=15, numClasses=3, numFeatures=4\n",
      "  If (feature 2 <= 2.449999988079071)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 2.449999988079071)\n",
      "   If (feature 3 <= 1.75)\n",
      "    If (feature 2 <= 5.1499998569488525)\n",
      "     If (feature 3 <= 1.6500000357627869)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 1.6500000357627869)\n",
      "      If (feature 0 <= 4.950000047683716)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 4.950000047683716)\n",
      "       Predict: 1.0\n",
      "    Else (feature 2 > 5.1499998569488525)\n",
      "     Predict: 2.0\n",
      "   Else (feature 3 > 1.75)\n",
      "    If (feature 2 <= 4.8500001430511475)\n",
      "     If (feature 0 <= 5.950000047683716)\n",
      "      Predict: 1.0\n",
      "     Else (feature 0 > 5.950000047683716)\n",
      "      Predict: 2.0\n",
      "    Else (feature 2 > 4.8500001430511475)\n",
      "     Predict: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt_model = dt.fit(train)\n",
    "# Display model summary\n",
    "print(\"Decision Tree model summary:{0}\".\\\n",
    "format(dt_model.toDebugString))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V0NJng6ZN4qV",
    "outputId": "7c43a8f3-5c82-411a-a494-2bd09ec42e7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9629629629629629, Precision: 0.9675925925925926, Recall: 0.9629629629629629, F1: 0.9632228719948018\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
