{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "outputId": "f77d5df9-8562-4c70-9ef5-68f5216c097b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "metadata": {
        "id": "379feb0b88a93d3a",
        "outputId": "361bd94b-3187-47f3-e849-819004c6e18d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,103 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,606 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,430 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,162 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,227 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,389 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,665 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,305 kB]\n",
            "Fetched 26.7 MB in 5s (5,066 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.5.2-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ],
      "execution_count": 2,
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Check this site for the latest download link\n",
        "# https://www.apache.org/dyn/closer.lua/spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j"
      ],
      "id": "379feb0b88a93d3a"
    },
    {
      "metadata": {
        "id": "96faff4e572f7e51",
        "outputId": "994433f4-9269-454c-b059-a9ad89f4d2c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset\n",
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|[5.09999990463256...|\n",
            "|  0.0|[4.90000009536743...|\n",
            "|  0.0|[4.69999980926513...|\n",
            "|  0.0|[4.59999990463256...|\n",
            "|  0.0|[5.0,3.5999999046...|\n",
            "|  0.0|[5.40000009536743...|\n",
            "|  0.0|[4.59999990463256...|\n",
            "|  0.0|[5.0,3.4000000953...|\n",
            "|  0.0|[4.40000009536743...|\n",
            "|  0.0|[4.90000009536743...|\n",
            "|  0.0|[5.40000009536743...|\n",
            "|  0.0|[4.80000019073486...|\n",
            "|  0.0|[4.80000019073486...|\n",
            "|  0.0|[4.30000019073486...|\n",
            "|  0.0|[5.80000019073486...|\n",
            "|  0.0|[5.69999980926513...|\n",
            "|  0.0|[5.40000009536743...|\n",
            "|  0.0|[5.09999990463256...|\n",
            "|  0.0|[5.69999980926513...|\n",
            "|  0.0|[5.09999990463256...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Train set\n",
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|[4.30000019073486...|\n",
            "|  0.0|[4.40000009536743...|\n",
            "|  0.0|[4.40000009536743...|\n",
            "|  0.0|[4.5,2.2999999523...|\n",
            "|  0.0|[4.59999990463256...|\n",
            "|  0.0|[4.59999990463256...|\n",
            "|  0.0|[4.59999990463256...|\n",
            "|  0.0|[4.69999980926513...|\n",
            "|  0.0|[4.69999980926513...|\n",
            "|  0.0|[4.80000019073486...|\n",
            "|  0.0|[4.80000019073486...|\n",
            "|  0.0|[4.80000019073486...|\n",
            "|  0.0|[4.80000019073486...|\n",
            "|  0.0|[4.90000009536743...|\n",
            "|  0.0|[4.90000009536743...|\n",
            "|  0.0|[4.90000009536743...|\n",
            "|  0.0|[4.90000009536743...|\n",
            "|  0.0|[5.0,3.0,1.600000...|\n",
            "|  0.0|[5.0,3.2000000476...|\n",
            "|  0.0|[5.0,3.2999999523...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Decision Tree model summary:\n",
            "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_8ba2f3e4aa59, depth=5, numNodes=15, numClasses=3, numFeatures=4\n",
            "  If (feature 2 <= 2.449999988079071)\n",
            "   Predict: 0.0\n",
            "  Else (feature 2 > 2.449999988079071)\n",
            "   If (feature 3 <= 1.75)\n",
            "    If (feature 2 <= 5.1499998569488525)\n",
            "     If (feature 3 <= 1.6500000357627869)\n",
            "      Predict: 1.0\n",
            "     Else (feature 3 > 1.6500000357627869)\n",
            "      If (feature 0 <= 4.950000047683716)\n",
            "       Predict: 2.0\n",
            "      Else (feature 0 > 4.950000047683716)\n",
            "       Predict: 1.0\n",
            "    Else (feature 2 > 5.1499998569488525)\n",
            "     Predict: 2.0\n",
            "   Else (feature 3 > 1.75)\n",
            "    If (feature 2 <= 4.8500001430511475)\n",
            "     If (feature 0 <= 5.950000047683716)\n",
            "      Predict: 1.0\n",
            "     Else (feature 0 > 5.950000047683716)\n",
            "      Predict: 2.0\n",
            "    Else (feature 2 > 4.8500001430511475)\n",
            "     Predict: 2.0\n",
            "\n",
            "+-----+--------------------+--------------+--------------------+----------+\n",
            "|label|            features| rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+--------------+--------------------+----------+\n",
            "|  0.0|[4.40000009536743...|[40.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|[4.59999990463256...|[40.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|[4.80000019073486...|[40.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|[5.0,3.4000000953...|[40.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|[5.0,3.4000000953...|[40.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|[5.0,3.5,1.600000...|[40.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|[5.0,3.5999999046...|[40.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|[5.09999990463256...|[40.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|[5.19999980926513...|[40.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  0.0|[5.69999980926513...|[40.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
            "|  1.0|[5.69999980926513...|[0.0,41.0,1.0]|[0.0,0.9761904761...|       1.0|\n",
            "|  1.0|[5.69999980926513...|[0.0,41.0,1.0]|[0.0,0.9761904761...|       1.0|\n",
            "|  1.0|[5.69999980926513...|[0.0,41.0,1.0]|[0.0,0.9761904761...|       1.0|\n",
            "|  1.0|[5.90000009536743...|[0.0,41.0,1.0]|[0.0,0.9761904761...|       1.0|\n",
            "|  1.0|[6.09999990463256...|[0.0,41.0,1.0]|[0.0,0.9761904761...|       1.0|\n",
            "|  1.0|[6.59999990463256...|[0.0,41.0,1.0]|[0.0,0.9761904761...|       1.0|\n",
            "|  1.0|[6.59999990463256...|[0.0,41.0,1.0]|[0.0,0.9761904761...|       1.0|\n",
            "|  2.0|[5.80000019073486...|[0.0,0.0,34.0]|       [0.0,0.0,1.0]|       2.0|\n",
            "|  2.0|[6.19999980926513...|[0.0,0.0,34.0]|       [0.0,0.0,1.0]|       2.0|\n",
            "|  2.0|[6.30000019073486...|[0.0,41.0,1.0]|[0.0,0.9761904761...|       1.0|\n",
            "+-----+--------------------+--------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "execution_count": 7,
      "source": [
        "# Import necessary modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.sql.types import StructType, StructField, FloatType, StringType\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "            .appName(\"Decision-Trees-Example\") \\\n",
        "            .config(\"spark.ui.port\", \"4040\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "# Define the schema of the iris dataset\n",
        "iris_schema = StructType([\n",
        "    StructField(\"Id\", FloatType(), True),\n",
        "    StructField(\"SepalLengthCm\", FloatType(), True),\n",
        "    StructField(\"SepalWidthCm\", FloatType(), True),\n",
        "    StructField(\"PetalLengthCm\", FloatType(), True),\n",
        "    StructField(\"PetalWidthCm\", FloatType(), True),\n",
        "    StructField(\"Species\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Load the iris dataset from Google Drive\n",
        "iris_df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"mode\", \"permissive\") \\\n",
        "    .schema(iris_schema) \\\n",
        "    .load(\"/content/drive/MyDrive/ColabNotebooks/datasets/Iris.csv\")\n",
        "\n",
        "# Index the Species column to convert string labels to numeric\n",
        "indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n",
        "iris_df = indexer.fit(iris_df).transform(iris_df)\n",
        "\n",
        "# Assemble the features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], outputCol=\"features\")\n",
        "data_with_features = assembler.transform(iris_df).select(\"label\", \"features\")\n",
        "\n",
        "# Split the data into training and test sets (80% training, 20% testing)\n",
        "train, test = data_with_features.randomSplit([0.8, 0.2], seed=13)\n",
        "\n",
        "# Show the dataset and training set\n",
        "print(\"Dataset\")\n",
        "data_with_features.show()\n",
        "\n",
        "print(\"Train set\")\n",
        "train.show()\n",
        "\n",
        "# Initialize and train the Decision Tree model\n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "\n",
        "# Train to get the model\n",
        "dt_model = dt.fit(train)\n",
        "\n",
        "# Display model summary\n",
        "print(\"Decision Tree model summary:\")\n",
        "print(dt_model.toDebugString)\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "predictions = dt_model.transform(test)\n",
        "\n",
        "# Show predictions\n",
        "predictions.show()\n",
        "\n",
        "# Evaluate the model using MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")"
      ],
      "id": "96faff4e572f7e51"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy, precision, recall, and F1 score\n",
        "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})"
      ],
      "metadata": {
        "id": "Fh6vEG9FRUk-"
      },
      "id": "Fh6vEG9FRUk-",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "id": "57dbb59407093890",
        "outputId": "5bb6bfac-4dbb-46a8-a36d-9298ca11ce93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9629629629629629\n"
          ]
        }
      ],
      "execution_count": 14,
      "source": [
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "id": "57dbb59407093890"
    },
    {
      "metadata": {
        "id": "ad89255b48003ff3",
        "outputId": "e699a44d-475c-4c76-dc86-750989561441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9675925925925926\n"
          ]
        }
      ],
      "execution_count": 15,
      "source": [
        "print(f\"Precision: {precision}\")"
      ],
      "id": "ad89255b48003ff3"
    },
    {
      "metadata": {
        "id": "dc9e8a7ac8449173",
        "outputId": "9629e7cb-8cb8-4a43-9171-7e9fb387d617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.9629629629629629\n"
          ]
        }
      ],
      "execution_count": 16,
      "source": [
        "print(f\"Recall: {recall}\")"
      ],
      "id": "dc9e8a7ac8449173"
    },
    {
      "metadata": {
        "id": "30955c8260a05848",
        "outputId": "0bac99c3-3a52-404d-b4e4-89db1df36f84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.9632228719948018\n"
          ]
        }
      ],
      "execution_count": 17,
      "source": [
        "print(f\"F1 Score: {f1}\")"
      ],
      "id": "30955c8260a05848"
    },
    {
      "metadata": {
        "id": "9bb0fbf132d164c3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 18,
      "source": [
        "# Stop Spark session\n",
        "spark.stop()"
      ],
      "id": "9bb0fbf132d164c3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}