{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "juTf1fBhRrS6"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.types import StructType, StructField, FloatType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Check this site for the latest download link\n",
        "# https://www.apache.org/dyn/closer.lua/spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvnbDGRjSIr-",
        "outputId": "a5a414c3-4b3e-4197-f3bf-1dfc2d5516d6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "59 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.5.2-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "            .appName(\"Decision-Trees-Example\") \\\n",
        "            .config(\"spark.ui.port\",\"4040\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")"
      ],
      "metadata": {
        "id": "EbzEqHVkR5U4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructField, StructType, FloatType, IntegerType, StringType\n",
        "\n",
        "iris_prediction_schema = StructType([\n",
        "    StructField(\"Id\", IntegerType(), True),\n",
        "    StructField(\"SepalLengthCm\", FloatType(), True),\n",
        "    StructField(\"SepalWidthCm\", FloatType(), True),\n",
        "    StructField(\"PetalLengthCm\", FloatType(), True),\n",
        "    StructField(\"PetalWidthCm\", FloatType(), True),\n",
        "    StructField(\"Species\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "pi-HiW70Ts96"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_df = spark.read.format(\"csv\").\\\n",
        "    option(\"header\", \"true\").\\\n",
        "    option(\"mode\", \"permissive\").\\\n",
        "    option(\"path\", \"/content/Iris.csv\").\\\n",
        "    schema(iris_prediction_schema).\\\n",
        "    load()\n",
        "iris_df.printSchema()\n",
        "iris_df.show(n=10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTEagJCLSG0E",
        "outputId": "f2522327-78c9-4254-9026-46cb1f7ec83b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: integer (nullable = true)\n",
            " |-- SepalLengthCm: float (nullable = true)\n",
            " |-- SepalWidthCm: float (nullable = true)\n",
            " |-- PetalLengthCm: float (nullable = true)\n",
            " |-- PetalWidthCm: float (nullable = true)\n",
            " |-- Species: string (nullable = true)\n",
            "\n",
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "|Id |SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species    |\n",
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "|1  |5.1          |3.5         |1.4          |0.2         |Iris-setosa|\n",
            "|2  |4.9          |3.0         |1.4          |0.2         |Iris-setosa|\n",
            "|3  |4.7          |3.2         |1.3          |0.2         |Iris-setosa|\n",
            "|4  |4.6          |3.1         |1.5          |0.2         |Iris-setosa|\n",
            "|5  |5.0          |3.6         |1.4          |0.2         |Iris-setosa|\n",
            "|6  |5.4          |3.9         |1.7          |0.4         |Iris-setosa|\n",
            "|7  |4.6          |3.4         |1.4          |0.3         |Iris-setosa|\n",
            "|8  |5.0          |3.4         |1.5          |0.2         |Iris-setosa|\n",
            "|9  |4.4          |2.9         |1.4          |0.2         |Iris-setosa|\n",
            "|10 |4.9          |3.1         |1.5          |0.1         |Iris-setosa|\n",
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Preprocessing 1: StringIndexer for categorical labels\n",
        "label_indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n",
        "data = label_indexer.fit(iris_df).transform(iris_df)"
      ],
      "metadata": {
        "id": "MxzMvO-8V6SH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.show(n=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhCNUAUfWzUm",
        "outputId": "a943104c-1035-4b7a-a84c-ae52be7018d8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+------------+-------------+------------+-----------+-----+\n",
            "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|label|\n",
            "+---+-------------+------------+-------------+------------+-----------+-----+\n",
            "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|  0.0|\n",
            "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|  0.0|\n",
            "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|  0.0|\n",
            "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|  0.0|\n",
            "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|  0.0|\n",
            "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|  0.0|\n",
            "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|  0.0|\n",
            "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|  0.0|\n",
            "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|  0.0|\n",
            "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|  0.0|\n",
            "+---+-------------+------------+-------------+------------+-----------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], outputCol=\"features\")\n",
        "data_with_features = assembler.transform(data).select(\"label\", \"features\")"
      ],
      "metadata": {
        "id": "zTV86ByyUaZJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets 80% training data and 20% testing data\n",
        "train, test = data_with_features.randomSplit([0.8, 0.2], seed=13)"
      ],
      "metadata": {
        "id": "qo1qsbc_U0tS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
        "# Initialize the LinearSVC classifier for binary\n",
        "# classification\n",
        "lsvc = LinearSVC(maxIter=10, regParam=0.01)\n",
        "\n",
        "# Set up OneVsRest classifier for multi-class\n",
        "# classification\n",
        "ovr = OneVsRest(classifier=lsvc)"
      ],
      "metadata": {
        "id": "JVlHycGi9gl3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# TRAIN\n",
        "# ============================\n",
        "\n",
        "# Train the model\n",
        "ovr_model = ovr.fit(train)"
      ],
      "metadata": {
        "id": "RjmaCvuRVQAp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# PREDICTIONS\n",
        "# ============================\n",
        "\n",
        "# Use the trained model to make predictions on the test data\n",
        "predictions = ovr_model.transform(test)\n",
        "\n",
        "# Show predictions\n",
        "predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GLw4H_rXg9K",
        "outputId": "01180999-915f-4090-d5ea-0f80462fd910"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+----------+\n",
            "|label|            features|       rawPrediction|prediction|\n",
            "+-----+--------------------+--------------------+----------+\n",
            "|  0.0|[4.40000009536743...|[1.90099210398048...|       0.0|\n",
            "|  0.0|[4.59999990463256...|[2.08594433990854...|       0.0|\n",
            "|  0.0|[4.80000019073486...|[1.76539206613239...|       0.0|\n",
            "|  0.0|[5.0,3.4000000953...|[2.39387775836876...|       0.0|\n",
            "|  0.0|[5.0,3.4000000953...|[2.10103921023010...|       0.0|\n",
            "|  0.0|[5.0,3.5,1.600000...|[2.01860384014608...|       0.0|\n",
            "|  0.0|[5.0,3.5999999046...|[2.76520695611947...|       0.0|\n",
            "|  0.0|[5.09999990463256...|[2.56056966826615...|       0.0|\n",
            "|  0.0|[5.19999980926513...|[3.55461864085416...|       0.0|\n",
            "|  0.0|[5.69999980926513...|[2.51096786077951...|       0.0|\n",
            "|  1.0|[5.69999980926513...|[-1.5025526835425...|       1.0|\n",
            "|  1.0|[5.69999980926513...|[-1.7004614620314...|       1.0|\n",
            "|  1.0|[5.69999980926513...|[-1.1084968734757...|       1.0|\n",
            "|  1.0|[5.90000009536743...|[-1.5609616212990...|       1.0|\n",
            "|  1.0|[6.09999990463256...|[-1.8525805280000...|       1.0|\n",
            "|  1.0|[6.59999990463256...|[-1.9824142058957...|       1.0|\n",
            "|  1.0|[6.59999990463256...|[-1.8442146370057...|       1.0|\n",
            "|  2.0|[5.80000019073486...|[-2.9320460867684...|       2.0|\n",
            "|  2.0|[6.19999980926513...|[-2.6155623835588...|       2.0|\n",
            "|  2.0|[6.30000019073486...|[-2.5029540543122...|       1.0|\n",
            "+-----+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "# Evaluate the model using MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Calculate precision\n",
        "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
        "print(f\"Precision: {precision}\")\n",
        "\n",
        "# Calculate recall\n",
        "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
        "print(f\"Recall: {recall}\")\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
        "print(f\"F1 Score: {f1}\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9uozcquXm8g",
        "outputId": "fe2529ca-81f6-49f8-846c-aa51b858989a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9629629629629629\n",
            "Precision: 0.9675925925925926\n",
            "Recall: 0.9629629629629629\n",
            "F1 Score: 0.9632228719948018\n"
          ]
        }
      ]
    }
  ]
}